---
title: ""
format: html
execute:
  cache: true
---

```{r echo = FALSE, include=F}
# rm(list = ls())
for(i in c("source/utils.R",
           "source/customized_sampleStat.R",
           "source/customized_Likelihood.R")) { source(i) }

# `mvtnorm` package is needed for normal density
# `gaussquad` is needed for Hermite-Gaussian quadrature
# `nlme::fdHess` is needed to compute Hessian matrix for SE
library(tidyverse)
library(mvtnorm)
library(gaussquad)
fdHess <- nlme::fdHess


# Load data and model matrix ----------------------------------------------
inp <- readRDS("data_and_modelSpec.RDS")


data <- inp$data 
# x1-x3: factor indicators
# x4 : Z
# x5 : x covariate
# y1 : outcome Y

inp_model <- list(
  matrices = inp$model$matrices,
  info = inp$model$info)
pars.start <- inp$pars.start


# ------------------------------------------------------
# Compute EM algorithm for LMS
# -------------------------------------------------------

# Argument inputs --------------------------------------
# These arguments come from `em` function in `nlsem` package

model = inp_model     # Model matrix
data = data          # input data
start =  pars.start  # Starting values

max.iter = 1000        # Maximum iteration for EM; default is 200
max.mstep = 1        # Iteration for mstep; default is 1
convergence = 0.01   # 
m = 16               # number of quadrature points

neg.hessian = FALSE  # for getting SE (For Hessian matrix)
optimizer = "nlminb" # Obtain MLE

max.singleClass = 1  
qml = F              # Alternative to LMS

# EM Estimation begins ----------------------------------------------------
## Name starting values (e.g., negative value for variances)  
par.new <- start

ll.ret <- NULL # Save likelihoods across iterations (from M step)
ll.new <- 0    # Likelihood every iteration (from M step)

num.iter <- 0
run <- TRUE

ll.old <- ll.new
par.old <- par.new # par.new will keep updated
  
model = model
parameters = par.old
dat = data
m = m

##### Compute E-step         <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
mod.filled <- fill_model(model = model, parameters = parameters)
```

```{css, echo=FALSE}
.two-columns {
  display: flex;
  justify-content: space-between;
}
.column {
  width: 45%;
}
```


First, I'd like to answer two major concerns we discussed last week.

## Two major concerns

**1. LMS is based on the interaction between two latent variables**

  - While KM initially introduced the LMS to address the interaction effect between two latent variables, it isn't strictly limited to two latent factors. One of these variables can indeed be an observed one. This is evident as many empirical studies (and simulation studies as well) specifically employ this method for non-uniform DIF, especially when using the MIMIC model for DIF detection.

  - So, I believe that using LMS for FLPS scenarios makes sense. 

**2. LMS is based on the normality assumption, but the treatment assignment violates it**

  -   When the normality assumption in LMS is violated, it causes problems. And it is confirmed by previous research.

  - But, things get different for the latent interaction between an latent variable and an observed variable (but the observed variable isn't normal like a binary case).

  -   I don't believe that the normality in observed variables is not necessary for LMS, because it is used as exogenous variables.

  -   I believe that this situation is the same as a regression model with a dummy variable, where the normality of the dummy variable isn't typically a concern.

  - Below is the regression model with two predictors; one is continuous and the other one is binary. When simulating 100 times, the interaction effects and other coefficients are not biased.

#### Typical regression with dummy variable and its interaction

```{r}

sim_res <- lapply(1:100, function(x) {
  dummy = rbinom(1000, 1, 0.5)
  x = rnorm(1000)
  y = 0.5*dummy + -0.3*x + 0.4*dummy*x + rnorm(1000, 0, 1)
  
  fit <- lm(y ~ dummy + x + dummy:x, data.frame(y, x, dummy))
  round(coef(fit),3)
})
sim_res <- do.call("rbind", sim_res)
colMeans(sim_res) # True : 0.5  -0.3   0.4
```


#### MVN estimator for the regression with dummy variable and its interaction

  -   This holds true even when employing a multivariate normal distribution to estimate a model where a dummy variable serves as one of the independent variables. This is demonstrated in the simulation results below, which were obtained using `lavaan` The package uses a multivariate normal distribution to fit all the variables in the model, including y, x, and the dummy variable.

  -   Multivariate normal distribution assumes that all variables follow a normal distribution; however, a dummy variable inherently violates this assumption.

  -   Despite the dummy variable not following a normal distribution, its non-normality does not impact the estimates as below.

  -   Thus, using a binary variable as an independent variable is still consistent with the normality assumption.

```{r}
# It is the same when using multivariate normal distribution
library(lavaan)

sim_res <- lapply(1:100, function(x) {
  
  dummy = rbinom(1000, 1, 0.5)
  x = rnorm(1000)
  y = 0.5*dummy + -0.3*x + 0.4*dummy*x + rnorm(1000, 0, 1)
  
  fit <- lavaan::sem(
    data = data.frame(y, x, dummy),
    model = 'y ~ dummy + x + dummy:x

            y ~~ y
            dummy ~~ dummy
            x ~~ x',
    meanstructure = T, fixed.x = F
    
  )
  est <- parameterestimates(fit) %>% 
    filter(lhs == "y", op == "~") %>% pull(est)
  names(est) <- c("dummy", "x", "dummy:x")
  est
})
sim_res <- do.call("rbind", sim_res)
colMeans(sim_res) # True : 0.5  -0.3   0.4

```

  - The same unbiased results are also supported by our simulation studies. 
  
  - Numerous other studies specifically utilize these interaction effects for non-uniform DIF detection.

  - I've also conducted a quick simulation on this matter, and the results appear unbiased. Please refer to [the Mplus output](https://raw.githubusercontent.com/sooyongl/customLMS/main/monte_binary.out) for details.


## LMS with one latent factor and a binary obervsed variable

Based on the KM 2000 paper, the following is my understanding of of how the LMS method addresses the latent interaction effect between a latent variable and an observed variable, irrespective of the type of observed variables.

While the original design of LMS focuses on interaction effects between two latent variables, this approach can seamlessly be extended to encompass interactions between one latent factor and an observed variable. Essentially, LMS does not impose constraints when incorporating an observed variable.

Binary variables like treatment assignments are treated as continuous variable like in a typical regression model with dummy variable.

For illustration, consider an estimation procedure that involves one latent variable (represented by 3 continuous indicators), a binary variable (Z; analogous to treatment status), and one covariate (x1). (Note: with binary indicators, things are a little bit different, I'll elaborate on this later)

This procedure follows Klein and Moosburger 2000, and the results are identical to Mplus.

My central point here is to show that treating an observed binary variable as continuous is valid, given its role as an "independent" variable.


## LMS specification

Our structural model can be re-formulated as:

$$
y = \alpha +
\begin{pmatrix} \omega & \tau_0 & \gamma_1  \\ \end{pmatrix}
\begin{pmatrix} \eta_1 \\ z \\ x_1 \\ \end{pmatrix} +
\begin{pmatrix} \eta_1 \\ z \\ x_1 \\ \end{pmatrix}^T
\begin{pmatrix} 0 & \tau_1 & 0 \\ 0 & 0 & 0  \\ 0 & 0 & 0  \\ \end{pmatrix}
\begin{pmatrix} \eta_1 \\ z \\ x_1 \\ \end{pmatrix} +
\varepsilon
$$

```{=tex}
\begin{align}
\eta_1 = \beta_1x_1 + \epsilon_{\eta_1} \\
\epsilon_{\eta_1} \sim N(0, \sigma_{\eta_1})
\end{align}
```

For simplicity, instead of $\beta_1$ regression, the covariance between $\eta_1$ and $x_1$ is estimated (meaning $\sigma_{\eta_1}$ directly indicating the factor variance instead of residual variance).

The measurement model is specified as:

```{=tex}
\begin{align}
item_1 = \lambda_{1} \eta_1 + \epsilon_{item_1} \\
item_2 = \lambda_{2} \eta_1 + \epsilon_{item_2} \\
item_3 = \lambda_{3} \eta_1 + \epsilon_{item_3} \\
\epsilon_{item} \sim N(0, \sigma_{item})
\end{align}
```

Again, the case where $item$ are binary items will be discussed later.

The exogenous variable vector $(\eta_1, z, x )$ can be decomposed using Cholesky decomposition (see Eq 8 from KM 2000; AC should be covariance matrix, but it was followed by KM 2000 notation).

$$
\begin{pmatrix}\eta \\ Z \\ x_1 \end{pmatrix} = 
\begin{pmatrix}
a_{11} & 0 & 0 \\
a_{21} & a_{22} & 0 \\
a_{31} & a_{32} & a_{32} \\
\end{pmatrix}
\begin{pmatrix}c_1 \\ c_2 \\ c_3 \end{pmatrix} = AC,
$$

where A is a lower triangle of Cholesky decomposition of the variance-covariance matrix of the exogenous variable vector (which will be represented by $\Phi$ = $AC$), and $c_1$ \~ $c_3$ are standardized normal variables, which are used to normalize the non-normal distribution. Specifically, in this case, there's only one latent interaction effects, $c_2$ and $c_3$ are constrained to zero; thus

$$
\begin{pmatrix}\eta \\ Z \\ x_1 \end{pmatrix} = 
\begin{pmatrix}
a_{11} & 0 & 0 \\
a_{21} & a_{22} & 0 \\
a_{31} & a_{32} & a_{32} \\
\end{pmatrix}
\begin{pmatrix}c_1 \\ 0 \\ 0 \end{pmatrix} = AC,
$$

Note that $c_1$ cannot be directly estimated; and, $c_1$ will be replaced with Hermite-Gaussian quadrature later in the procedure.

To simplify,

$$
y = \alpha + \Gamma' AC + C'A'\Omega AC + \epsilon
$$ 

\begin{align}
\mathbf{item} = \Lambda \eta_1 + \epsilon_{item} \\
\epsilon_{item} \sim N(0, diag(\sigma_{item}))
\end{align}


## Model implied mean and covariance

The model implied mean vectors and covariance below will follow multivariate normal distribution based on LMS specification. These components will be used to multivariate normal density function to compute likelihood values.

The following equations come from Eq.18 \~ Eq.22 (KM 2000).

First,

```{=tex}
\begin{align}
\Lambda_x = \begin{pmatrix}
\lambda_1 & 0 & 0 \\ 
\lambda_2 & 0 & 0 \\ 
\lambda_3 & 0 & 0 \\ 
0 & 1 & 0 \\ 
0 & 0 & 1 
\end{pmatrix} && 
\Gamma = \begin{pmatrix}
\omega \\ \tau_0 \\ \gamma 
\end{pmatrix} && 
\Omega = \begin{pmatrix} 0 & \tau_1 & 0 \\ 0 & 0 & 0  \\ 0 & 0 & 0  \\ \end{pmatrix} \\

A =
\begin{pmatrix}
a_{11} & 0 & 0 \\
a_{21} & a_{22} & 0 \\
a_{31} & a_{32} & a_{32} \\
\end{pmatrix}, && 
C_1 = \begin{pmatrix}
c_1 \\ 0 \\ 0 
\end{pmatrix} &&
I_{\text{block}} = \begin{pmatrix}
0 & 0  \\
0 & I_{n-k} \\
\end{pmatrix} \\

\Lambda_y = \begin{pmatrix}
1
\end{pmatrix} &&

\mathbf{\nu_{x}} = \begin{pmatrix}
\nu_{item_1}  \\ 
\nu_{item_2}  \\ 
\nu_{item_3}  \\ 
\nu_{z}  \\ 
\nu_{x_1}  
\end{pmatrix} &&

\Theta = \begin{pmatrix}
\sigma^2_{item_1} & 0 & 0 & 0 & 0\\ 
0 & \sigma^2_{item_2} & 0 & 0 & 0 \\ 
0 & 0 & \sigma^2_{item_3} & 0 & 0 \\ 
0 & 0 & 0 & 0 & 0  \\ 
0 & 0 & 0 & 0 & 0
\end{pmatrix}


\end{align}
```
$$
\mathbf{\tau} = 
\begin{pmatrix}
\tau_{eta_1} \\
\tau_{z} \\
\tau_{x_1} \\
\end{pmatrix}
$$

$$
T = \Gamma A + C_1' A'\Omega A
$$

**Note.**

-   $\nu_{z}$ and $\nu_{x_1}$ are fixed at the means of $z$ and $x_1$ (no need for estimation); but $\nu_{item}$ serve as item intercepts.

-   $\tau$ is the mean of latent factors; For the observed variables, their means will be fed to $\tau_z$ and $\tau_{x_1}$.

-   $\sigma^2_{z}$ and $\sigma^2_{x_1}$ are zero; $\sigma_{zx}$ is zero; instead, $\Phi$ represents those parameters (but not estimated, because they are observed variables; no need to estimate them).

-   $n$ equals number of covariates including z (3 in this example; $\eta$, z, x1)

-   $k$ = number of interaction effects (1 in this example; $\tau_1$: effect between $\eta_1$ and z)

-   $I_{\text{block}}$ in this example will be $\begin{pmatrix} 0 & 0 & 0 \\ 0 & 1 & 0 \\ 0 & 0 & 1 \end{pmatrix}$.

<!-- - $\kappa_{y}$ is the intercept of $y$, but is constrained to zero -->

### Means of indicators of exogenous factors

$$
\mu_x(c_1) = \nu_x +
\Lambda_x
\mathbf{\tau}
A
C_1
$$

### Means of indicators of endogenous factor

$$
\mu_y(c_1) = \nu_y + \Lambda_y(\alpha + \Gamma (\mathbf{\tau} +A C_1) + (\mathbf{\tau} +A C_1)'\Omega (\mathbf{\tau} +A C_1) )
$$

- with  $(\mathbf{\tau} +A C_1)$ = $tAc$

-   Here, we have one observed variable for endogenous variable;

-   It is treated as a latent factor with a single variable; with the mean and variance of the indicator fixed to zero; ($\nu_y$ == 0)


### Covariance matrix of exogenous indicators

$$
\Sigma_{xx}(c_1) = 
\Lambda_x A I_{block} A' \Lambda_x' + \Theta
$$

### Covariance matrix between exogenous and endogenous indicators

$$
\Sigma_{xy}(c_1) = \Lambda_xA
I_{\text{block}}
T'
$$

### Covariance matrix of endogenous indicators

$$ 
\Sigma_{yy}(c1) = \Lambda*y( T I_{\text{block}} T')\Lambda\_y' +
\Lambda*y*\Psi{y}\Lambda\_y' + \theta\_y
$$

-   Again, we have only observed variable for endogenous factor.
-   $\theta_y$ is fixed to zero; instead $\Psi_{y}$



The above model-implied $\mu$ and $\Sigma$ are fed into the multivariate normal density, $\phi$. The density value for the realizaiton (x, y) across k-dimensional real space $R^m$ is

$$
f(x, y) = \int_{R^m} \phi(c_1)\phi_{\mu(c_1),\Sigma(c_1)} (x,y)  \,dc_1
$$


## Estimation

-   The parameter matrices listed below align with the LMS specification mentioned earlier.
-   The element NA represents the parameter that is to be estimated.
-   Note. Theta.d = $\Theta$, Theta.e = $\theta_y$

```{r echo = F}
mat <- inp_model$matrices$class1
mat
```


With the starting values, EM algorithm starts.

```{r echo = F}
mod.filled$matrices$class1
```


Because of the nonlinear relationship mentioned above, the integral of the mixture density cannot be solved analytically.

The $m$-dimensional integral above can be approximated by numerical methods, for example, by application of Hermite-Gaussian quadrature formulas s for numerical integration (Isaacson & Keller, 1966).

To apply Hermite-Gaussian quadrature formulas, the integration variable $c_1$ is substituted by

$$
u = 2^{-1/2}c_1
$$

The $m$-dimensional integral above is approximated by iterated application (k times) of the Hermite-Gaussian quadrature formula, which approximates one-dimensional integrals with weight function $exp(-u^2)$ in the integrand by finite sums.


The quadrature formula provides M weights w_j \<- probability 'p' in my code.

$m$ is quadrature points, and 16 here (larger should better approximate the distribution).

$$
f(x, y) = \int_{R^m} \pi^{-m/2}exp(-u'u)\phi_{\mu(2^{-1/2}c_1),\Sigma(2^{-1/2}c_1)} (x,y)  \,du
$$

$u = 2^{-1/2}c_1$ =


```{r}
# Calculate Hermite Quadrature <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
k <- 1 # Number of interaction effects
quad <- quadrature(m, k)

u <- quad$n
u
```

$w=\pi^{-m/2}exp(-u'u)$

```{r}
w <- quad$w
# Hermite Quadrature Ends <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
w
```


With some definitions (p. 465), the above integral can be reduced to the standard notation for a finite mixture density: 


$$
f(x, y) \approx \sum_{j=1}^{M} w_j\phi_{\mu_u,\Sigma_u} (x,y)
$$
Here, $w_j$ is the same as $\rho_j$ in Eq 29 in KM 2000.


## Compute E-step

Followed by p. 466,

$$
p^{(r)}(j=j|x_i, y_i) = \frac{w_j\phi(x,y)}{f(x_i, y_i)}
$$

- matrix $P^{(r)}$ contains the posterior probabilities for the mixture components j given the i-th row of the data matrix

-   Finally, in this step, we can get mixture proportions, `P`, which will be used in M-step.

```{r}

# Compute p  <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
P <- NULL
for (i in seq_along(w)) {
  # i = 1
  p.ij <- w[i] * mvtnorm::dmvnorm(dat, 
                                  mean = MU_lms(mod.filled, u[i, ]),
                                  sigma = SIGMA_lms(mod.filled, u[i, ]))
  P <- cbind(P, p.ij, deparse.level = 0)
}
P <- P/rowSums(P)
head(P) # N x m matrix where N is the number of individuals.
# E-step Ends         <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
```

-   `MU_lms` and `SIGMA_lms` compute the model-implied mean and variance shown above at every $C_1$ (or $u$) (refer to [this](https://github.com/sooyongl/customLMS/blob/main/source/customized_sampleStat.R))


```{r eval = F}

  matrices <- mod.filled$matrices$class1
  matrices$A <- t(chol(matrices$Phi))
  n <- nrow(matrices$A) # Number of covariates
  if (k < n & k > 0) {
    z.1 <- c(z, rep(0, n - k)) # Note: z.1 is C.1; the original notation is z, so I won't change it. 
                               # in our notation, z.1 is C_1 vector. (C_1, 0, 0)
                               # Again, n and k is the number of exogenous variables and the number of interaction effects.
  }
  
  A.z <- matrices$A %*% z.1 # A.C 
  
```

Inside `MU_lms` and `SIGMA_lms`, $\Phi$ (covariance matrix of exogenous variables) are converted to $A$ and given $C_1$ vector; `A.C = A %*% c.1`


## Compute M-step

-   Below is computing the likelihood for M-step using mixture proportion (refer to [this](https://github.com/sooyongl/customLMS/blob/main/source/customized_Likelihood.R)).


$$
\theta^{(r)} = arg \max_\theta\{\sum_{i=1}^N \sum_{j=1}^{M} p^{(r)}(j=j|x_i,y_i)ln\phi_{\mu_{c_1},\Sigma_{c_1}})(x_i, y_i)\}
$$

```{r}
k <- 1
quad <- quadrature(m, k)
V <- quad$n # quadrature points

res0 <- sapply(seq_len(nrow(V)), function(i) { # i = 1
  lls <- sum(
    P[, i] *
    mvtnorm::dmvnorm(dat,
                     mean = MU_lms(model = mod.filled, z = V[i, ]),
                     sigma = SIGMA_lms(model = mod.filled, z = V[i, ]),
                     log = TRUE) )
  lls
})
res <- sum(res0)
-res
```

-   The estimates that maximize the given likelihood will continue to update the starting values until there is no discernible difference between the new likelihood and the previous one (e.g., a difference of 0.001).


## Cases for binary  indicators

For scenarios where $item$ is binary items (like our cases), things get more complicated.

Using latent response variable framework (Muthén & Asparouhov, 2002) $item^*$ is used instead of $item$ for multivariate normal distribution with some more constraints (i.g., $E(item^*)=0$ and $V(item^*)=1$. 

<!-- The relationship between binary indicators and other variables is estimated by tetrachoric correlation, polyserial correlation (point-biserial in our case).  -->

<!-- These correlations are used to compute the likelihood values.  -->

$item^*$ will be dichotomized by threshold estimates, with the appropriate link function (e.g., logit or probit). 

Due to some complication to make estimator for categorical indicators, I stop here. However the general procedure is consistent with the continuous indicators.

Muthén, B., & Asparouhov, T. (2002). Latent variable analysis with categorical outcomes: Multiple-group and growth modeling in Mplus. Mplus web notes, 4(5), 1-22.

## Take away

- While the initial LMS concept by KM was centered around the interaction effect between two latent variables, it doesn't strictly bind to such a structure. The model can easily accommodate an observed variable in the mix. In fact, numerous empirical studies have adopted this approach, especially when detecting non-uniform DIF using the MIMIC model.

- Again, I believe that using LMS for our FLPS scenario doesn't violate its assumption.

- I hope that above could address all the concerns related to LMS.




